{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d42ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import psycopg2\n",
    "\n",
    "from faker import Faker\n",
    "faker = Faker()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d889c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE DON'T WANT OUR TERMINAL DIRTIED WITH ERROR MESSAGES BUT STILL KEEP A GOOD WATCH ON THEM\n",
    "# SO WE WRITE WITH BEST PRACTICES AT HEART, WE LOG\n",
    "import logging \n",
    "\n",
    "logger =  logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger_handler = logging.FileHandler('./logs/using_jupyter.log')\n",
    "logger_formatter = logging.Formatter('%(asctime)s -%(name)s %(levelname)s - %(message)s \\n')\n",
    "logger_handler.setFormatter(logger_formatter)\n",
    "logger.addHandler(logger_handler)\n",
    "logger.info('Logs is instatiated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f384f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLLECTING OUR CREDENTIALS FROM `connection_profile.conf` WITH `configparser`\n",
    "import configparser\n",
    "\n",
    "parser = configparser.ConfigParser()\n",
    "parser.read('./login_credentials/connection_profile.conf')\n",
    "\n",
    "postgres_host = parser.get('postgres_connection', 'host')\n",
    "postgres_port = parser.get('postgres_connection', 'port')\n",
    "postgres_database = parser.get('postgres_connection', 'database')\n",
    "postgres_user = parser.get('postgres_connection', 'user')\n",
    "postgres_password = parser.get('postgres_connection', 'password')\n",
    "postgres_table1 = parser.get('postgres_connection', 'table1')\n",
    "postgres_table2 = parser.get('postgres_connection', 'table2')\n",
    "\n",
    "mysql_host = parser.get('mysql_connection', 'host')\n",
    "mysql_port = parser.get('mysql_connection', 'port')\n",
    "mysql_database = parser.get('mysql_connection', 'database')\n",
    "mysql_user = parser.get('mysql_connection', 'user')\n",
    "mysql_password = parser.get('mysql_connection', 'password')\n",
    "mysql_table = parser.get('mysql_connection', 'table')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb774a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONNECTIING TO OUR LOCAL MYSQL DATABASE WITH `pymysql`\n",
    "\n",
    "try:\n",
    "    conn = pymysql.connect(host=mysql_host,\n",
    "                            port=int(mysql_port),\n",
    "                            user=mysql_user,\n",
    "                            password=mysql_password)                            \n",
    "except Exception:\n",
    "        logger.error('Error connecting to database', exc_info=True)\n",
    "\n",
    "else:\n",
    "    print('Connection successful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baae6b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING DATABASE\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    try:\n",
    "        cur.execute(f'CREATE DATABASE IF NOT EXISTS {mysql_database}')\n",
    "    except Exception:\n",
    "        logger.error(f'Error creating database-- {mysql_database}', exc_info=True)\n",
    "    else:\n",
    "        conn.commit()\n",
    "        print(f'Database-- {mysql_database} created successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a04faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING CREATED DATABASE\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    try:\n",
    "        cur.execute(f'USE {mysql_database}')\n",
    "    except Exception:\n",
    "        logger.error(f'Error using database-- {mysql_database}', exc_info=True)\n",
    "    else:\n",
    "        conn.commit()\n",
    "        print(f'Connected to Databse-- {mysql_database} succesfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e5092a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROPPING TABLE IF EXIST TO AVOID ERROR THROWBACK IF TABLE DOES NOT EXIST\n",
    "\n",
    "query1 =f'DROP TABLE IF EXISTS {mysql_table};'\n",
    "with conn.cursor() as cur:\n",
    "    try:\n",
    "        cur.execute(query1)\n",
    "    except Exception as err:\n",
    "        logger.error(f'Drop table-- {mysql_table}', exc_info=True)\n",
    "        print(err)\n",
    "    else:\n",
    "        conn.commit()\n",
    "        print(f'Dropping table-- {mysql_table} if exist')\n",
    "        \n",
    "# CREATING OUR TABLE SCHEMA\n",
    "\n",
    "query2 = f'CREATE TABLE {mysql_table} (first_name text(50), last_name text(50), ssn varchar(30),\\\n",
    "                                        home_address varchar(250), crypto_owned text(50), wage text(20),\\\n",
    "                                        phone_number text(40), company_worked text(150), bank_account text(70),\\\n",
    "                                        occupation text(50), company_mail varchar(70), personal_email varchar(70),\\\n",
    "                                        birth_date varchar(20));'\n",
    "with conn.cursor() as cur:\n",
    "    try:\n",
    "        cur.execute(query2)\n",
    "    except Exception as err:\n",
    "        logger.error(f'Error creating table-- {mysql_table}', exc_info=True)\n",
    "        print(err)\n",
    "    else:\n",
    "        conn.commit()\n",
    "        print(f'Creating table-- {mysql_table} successful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff45f257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAKER GENERATING OUR DATA\n",
    "\n",
    "data = []\n",
    "for i in range(500):\n",
    "    data.append((faker.first_name(), faker.last_name(), faker.ssn(),\\\n",
    "                faker.address(), faker.cryptocurrency_name(), faker.pricetag(),\\\n",
    "                faker.phone_number(), faker.company(), faker.bban(), faker.job(),\\\n",
    "                faker.company_email(), faker.ascii_free_email(), faker.date()))\n",
    "    i+= 1\n",
    "\n",
    "data_for_db = tuple(data)\n",
    "\n",
    "# INSERTING MUTIPLE ROWS WITH `executemany()`\n",
    "\n",
    "query2 = f\"INSERT INTO {mysql_table} VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "with conn.cursor() as cur:\n",
    "    try:\n",
    "        cur.executemany(query2, data_for_db)\n",
    "    except Exception:\n",
    "        logger.error('Error inserting into table-- {mysql_table}', exc_info=True)\n",
    "    else:\n",
    "        conn.commit()\n",
    "        print(f'Inserting into table-- {mysql_table} successful')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9666a7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLOSING OUR `conn`, THIS WAS HANDLED IN THE `.py` BY THE CONTEXT MANAGER\n",
    "\n",
    "if conn is not None:\n",
    "    try:\n",
    "        conn.close()\n",
    "        print('Connection closed')\n",
    "    except:\n",
    "        logger.error('Connection previously closed', exc_info=True)\n",
    "        print('Connection previously closed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aa98f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE NEXT THING WOULD BE TO CREATE AND CONNECT TO OUR DESTINATION POSTGRES DATABASE\n",
    "# THE CODE BELOW USING `psycopg2` AS OUR DATABASE ADAPTER IS SUPPOSE TO DO THE JOB BUT UNFORTUNATELY WOULD NOT WORK, HERE ARE THE VARIOUS REASONS:\n",
    "# 1 -- POSTGRES/PSYCOPG2 DOES NOT ALLOW THE CREATION OF 'DATABASE' WITHIN A TRANSACTION BLOCK, WHICH MEANS A DATABSE IS REQUIRED BY DEFAULT WITH PSYCOPG2\n",
    "#      trying would yield an error ::: \"can't create database with a transcation block\"\n",
    "# 2-- POSTGRES DOES NOT ALLOW DDL COMMANDS LIKE CREATING 'DATABASE' EASILY, DOING IT WOULD REQUIRE US TO EDIT \n",
    "#     THE POSTGRES APPLICATION SETTING SECURITY CONFIGURATION (`pg_hba.conf` file), AND I REALLY DON'T WANT US MESSING WITH THAT\n",
    "#      trying would yield an error ::: \"requires a peer authentication\"\n",
    "\n",
    "\n",
    "# PLS LOOK UP THE INTERNET ON CREATING A DATABASE WITH POSTGRES EITHER WITH `psql` OR `pgAdmin`, IT A VERY SIMPLE PROCESS\n",
    "\n",
    "\n",
    "# with psycopg2.connect(f'host={postgres_host} port={postgres_port} user={postgres_user} password={postgres_password}') as conn:\n",
    "#     try:\n",
    "#         with conn.cursor() as cur:\n",
    "#             cur.execute(f'CREATE DATABASE {postgres_database}')\n",
    "#             conn.commit()\n",
    "#             cur.execute(f'USE {postgres_database}')\n",
    "#             conn.commmit()\n",
    "#     except:\n",
    "#         logger.error('Error creating and using Postgres database--{postgres_databse}', exc_info=True)\n",
    "#     else:\n",
    "#         print(f'Creating database-- {postgres_database} succesfully')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bb07d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS MARK THE END OF 'SOURCING_AND_SETTING_UP_DB.py'. THE NEXT LINE IS CONCERNED WITH THE 'EtLT'/`ETL` OF OUR DATA REPLICATION PROCESS, I HOPE IT HAS BEEN CLEAR SO FAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65be5763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc5b6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b0d56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE JARS FILE ENABLES THE CONNECTION TO THE LOCAL INSTANCE RDBMS, CHECK THE TEXT FILE IN THE JAR FOLDER FOR THIER RESPECTIVE SOURCE\n",
    "# NOTE!!!, THE CONFIGURATION WORKED BEACAUSE THIS PARTICULAR SCRIPT IS EXPECTED TO BE RUN ON A SPARK STANDALONE CLIENT(i.e YOUR LOCAL MACHINE)\n",
    "# IN THE CASE OF MULTIPLE NODES, CONFIGURATION WOULD INCLUDE 'spark.extraClassPath.Driver' & 'spark.executor.Driver', PLS CHECK THE INTERNET FOR BETTER UNDERSTANDING ON THAT.\n",
    "# AND FIGURING THIS OUT HAD IT OWN SUBTLETY, I MUST TELL YOU\n",
    "\n",
    "spark = SparkSession.builder.config('spark.jars', './jars/mysql-connector-java-8.0.29.jar,./jars/postgresql-42.3.6.jar')\\\n",
    "        .appName('using_jupyter').enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0478e0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c85bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    " #  EXTRACTION \n",
    "\n",
    " # EXTRANTING FROM OUR LOCAL MYSQL RDBMS USING THE LOAD-METHOD, THERE IS A JDBC-METHOD, VERY MUCH SIMILAR\n",
    "try:\n",
    "    jdbcDF = spark.read.format('jdbc')\\\n",
    "                        .option('url', f'jdbc:mysql://{mysql_host}:{mysql_port}/{mysql_database}')\\\n",
    "                        .option('driver', 'com.mysql.jdbc.Driver')\\\n",
    "                        .option('dbtable', f'{mysql_table}')\\\n",
    "                        .option('user', f'{mysql_user}')\\\n",
    "                        .option('password', f'{mysql_password}').load()\n",
    "except:\n",
    "    logger.error(f'Eror extracting from {mysql_database}.{mysql_table} with pyspark', exc_info=True)\n",
    "else:\n",
    "    print(f'Successfully extracted data from database.table-- {mysql_database}.{mysql_table}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79df4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcDF.show(5)\n",
    "jdbcDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae55724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS TO WIDEN THE VIEW OF JUPYTER NOTEBOOK IN YOUR BROWSER\n",
    "\n",
    "from IPython.display import display, HTML \n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0279f66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMATION\n",
    "\n",
    "# USING PYSQARK TO DROP COLUMN `SSN`, WE ARE CONCLUDING IT PRIVATE INFORMATION...smile (it fake data)\n",
    "\n",
    "jdbcDF =  jdbcDF.drop('ssn')\n",
    "\n",
    "jdbcDF.printSchema()\n",
    "jdbcDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e556698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE'RE CHANGING THE COLUMN `DATE_OF_BIRTH` DATA TYPE FROM STRING TO DATE TYPE`\n",
    "\n",
    "jdbcDF = jdbcDF.withColumn('birth_date', to_date(col('birth_date'), 'yyyy-MM-dd'))\n",
    "\n",
    "jdbcDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65fd808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LET GET DOWN WITH SQL\n",
    "# CREATING A TEMP VIEW FROM OUR DATAFRAME\n",
    "\n",
    "jdbcDF.createOrReplaceTempView('temp_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7e79d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHOWING THE TEMP TABLE `TEMP_TABLE`\n",
    "\n",
    "spark.sql(\"SELECT T.* FROM TEMP_TABLE T\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcfcdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BREAKING OUR TEMP TABLE TO PERSONAL AND BUSSINESS INFORMATION AND CHANGING COLUMN NAME\n",
    "\n",
    "personal_info_tbl =  spark.sql(\"SELECT `first_name` AS `first name`, `last_name` AS `last name`, `birth_date` AS `date_of_birth`, `home_address`, `phone_number`,  `personal_email` FROM TEMP_TABLE\")\n",
    "business_info_tbl =  spark.sql(\"SELECT CONCAT(`first_name`,'   ', `last_name`) AS `full_name`, `occupation` AS `job`, `company_worked` AS `company`, `wage` AS `hourly/weekly_wage`, `crypto_owned`, `bank_account`, `company_mail` FROM TEMP_TABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72509c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_info_tbl.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0545954",
   "metadata": {},
   "outputs": [],
   "source": [
    "business_info_tbl.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e1544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD\n",
    "\n",
    "# AT THE FINAL TASK, OUR TWO SEPARATE TABLE IS LOADED TO OUR POSTGRES DATABASE\n",
    "try:\n",
    "      personal_info_tbl.write.format('jdbc').mode('append')\\\n",
    "                                            .option('url', f'jdbc:postgresql://{postgres_host}:{postgres_port}/{postgres_database}')\\\n",
    "                                            .option('dbtable', f'public.{postgres_table1}')\\\n",
    "                                            .option('user', f'{postgres_user}').option('driver', 'org.postgresql.Driver')\\\n",
    "                                            .option('password', f'{postgres_password}').save()\n",
    "except:\n",
    "    logger.error(f'Eror loading to {postgres_database}.public.{postgres_table1} with pyspark', exc_info=True)\n",
    "else:\n",
    "    print(f'Successfully loaded data to databse.public.table-- {postgres_database}.public.{postgres_table1}')     \n",
    "\n",
    "    # THE APPEND METHOD IS NOT REQUIRED FOR DATA REPLICATION, I JUST INCLUDED IT TO SHOW THERE IS SUCH, THE `.py` SCRIPTS WOULD HAVE IT                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a570737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "     # THE IS THE `jdbc` METHOD, AN ALTERNATIVE TO THE `save` METHOD\n",
    "                    business_info_tbl.write.jdbc(f'jdbc:postgresql://{postgres_host}:{postgres_port}/{postgres_database}', f'public.{postgres_table2}', 'append',\\\n",
    "                    properties={'user':f'{postgres_user}', 'password': f'{postgres_password}', 'driver': 'org.postgresql.Driver'})\n",
    "except:\n",
    "    logger.error(f'Eror loading to {postgres_database}.public.{postgres_table2} with pyspark', exc_info=True)\n",
    "else:\n",
    "    print(f'Successfully loaded data to databse.public.table-- {postgres_database}.public.{postgres_table2}')    \n",
    "\n",
    "    # THE APPEND METHOD IS NOT REQUIRED FOR DATA REPLICATION, I JUST INCLUDED IT TO SHOW THERE IS SUCH, THE `.py` SCRIPTS WOULD HAVE IT     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46580139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS WOULD HAVE WORTH THE WORK IF EVERY LINE OF CODE, DECISION AND REASIONING WAS OPEN AS BOOK TO YOU. IF ANY CORRECTION OR RECOMMENDATION, LET ME KNOW @ oluwatobitobias@gmail.com. THANKS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apache_airflow",
   "language": "python",
   "name": "apache_airflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
